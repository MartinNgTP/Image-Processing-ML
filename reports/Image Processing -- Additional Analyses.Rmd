---
title: "Image Processing:  Additional Analyses"
author: "Data Science Three Co."
date: "14-11-2024"
output: html_document
---

```{r include=FALSE}
rm(list = ls())
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r seed}
set.seed(41)
```

```{r libraries, message=FALSE, include=FALSE}
library(data.table)
library(DT)
library(class)    # For KNN function
library(caret)    # For confusionMatrix function
library(e1071)
library(randomForest)  # For Random Forest model
library(xgboost) # For XG boost
library(ggplot2)
library(reshape2)  # For reshaping the confusion matrix into a long format
library(glmnet) # For elastic net 
library(dplyr)
library(knitr)
library(magrittr)
library(nnet) # For Multinomial logistic regression and Neural Networks
```

```{r constants}
# Set up constants

n.values <- 5000
n.values.normal <- c(2500, 5000, 10000)
k <- 5  # Number of neighbors for KNN
sample_size <- 5000  # Define a sample size for the experiment
total_train_size <- 60000  # Total rows in the training set
hour <- 60  # To convert runtime to minutes
dp <- 4      # Decimal Places
rf_ntree <- 100 # RF ntree
xgboost_max_depth <- 6  # Define max depth for the model
xgboost_eta <- 0.3      # Learning rate for boosting
xgboost_nrounds <- 100  # Number of rounds for boosting
class_names <- c("Ankle boot", "Bag", "Coat", "Dress", "Pullover", "Sandal", "Shirt", "Sneaker", "T-shirt/top", "Trouser")
iterations <- 3


```

```{r functions}
round.numerics <- function(x, digits){
  if(is.numeric(x)){
    x <- round(x = x, digits = digits)
  }
  return(x)
}

# Function to calculate score
calculate_score <- function(A, B, C) {
  return(0.15 * A + 0.1 * B + 0.75 * C)
}

# Function to plot confusion matrix heatmap
plot_confusion <- function(matrix) {
  conf_matrix_table <- matrix$table
  conf_matrix_df <- as.data.frame(conf_matrix_table)
  colnames(conf_matrix_df) <- c("Reference", "Prediction", "Frequency")
 
  ggplot(conf_matrix_df, aes(x = Prediction, y = Reference)) +
    geom_tile(aes(fill = Frequency), color = "white") +
    scale_fill_gradient(low = "white", high = "steelblue") +
    labs(title = "Confusion Matrix Heatmap", x = "Predicted Class", y = "Actual Class", fill = "Frequency") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

# Function to print "byClass" statistics as datatable
byClass <- function(matrix) {
  # Extract the "byClass" statistics as a data frame
  stats_by_class <- as.data.frame(conf_matrix$byClass)

  # Rename row names (classes) into a new column
  stats_by_class$Class <- rownames(stats_by_class)
  rownames(stats_by_class) <- NULL  # Remove row names after extracting
  stats_by_class$Class <- class_names

  # Reorder columns to have "Class" first for readability
  stats_by_class <- stats_by_class[, c(ncol(stats_by_class), 1:(ncol(stats_by_class) - 1))]
  
  # Display the data frame as an interactive DataTable
  datatable(stats_by_class, options = list(
    pageLength = 10,         # Show 10 rows per page
    autoWidth = TRUE,
    dom = 'tB'             # Show only the table and export buttons
  )) %>%
    formatRound(columns = 2:ncol(stats_by_class), digits = 4)  # Format numeric columns to 4 decimal places
}

# SVM
# Function to train and evaluate SVM
run_svm <- function(sample_data, cost = 1, kernel = "linear") {
  sample_data <- as.data.frame(sample_data)
  sample_data$label <- as.factor(sample_data$label)
  
  # Measure runtime for SVM
  start_time <- Sys.time()
  svm_model <- svm(label ~ ., data = sample_data, kernel = kernel, cost = cost)
  svm_predictions <- predict(svm_model, newdata = test_features)
  runtime <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
  
  # Calculate accuracy and misclassification rate
  accuracy <- mean(svm_predictions == test_labels)
  return(list(accuracy = accuracy, runtime = runtime))
}

# RF
# Function to train and evaluate Random Forest
run_random_forest <- function(sample_data, ntree = rf_ntree) {
  sample_data <- as.data.frame(sample_data)
  
  # Measure runtime for Random Forest
  start_time <- Sys.time()
  rf_model <- randomForest(label ~ ., data = sample_data, ntree = ntree)
  rf_predictions <- predict(rf_model, newdata = test_features)
  runtime <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
  
  # Calculate accuracy and misclassification rate
  accuracy <- mean(rf_predictions == test_labels)
  return(list(accuracy = accuracy, runtime = runtime))
}

# Function to train and evaluate Neural Network
run_neural_net <- function(sample_data, size = 10) {
  sample_data <- as.data.frame(sample_data)
  
  # Measure runtime for Neural Network
  start_time <- Sys.time()
  nn_model <- nnet(label ~ ., data = sample_data, size = size, maxit = 200, trace = FALSE)
  nn_predictions <- predict(nn_model, newdata = test_features, type = "class")
  runtime <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
  
  # Calculate accuracy and misclassification rate
  accuracy <- mean(nn_predictions == test_labels)
  return(list(accuracy = accuracy, runtime = runtime))
}

# xgboost
# Function to train and evaluate GBM using xgboost
run_gbm <- function(sample_data, max_depth = xgboost_max_depth, eta = xgboost_eta, nrounds = xgboost_nrounds) {
  # Prepare data for xgboost
  train_matrix <- xgb.DMatrix(data = as.matrix(sample_data[, -1, with = FALSE]), label = as.numeric(sample_data$label) - 1)
  test_matrix <- xgb.DMatrix(data = as.matrix(test_features), label = as.numeric(test_labels) - 1)
  
  # Set parameters for xgboost
  params <- list(
    objective = "multi:softmax",
    num_class = length(unique(train$label)),
    max_depth = max_depth,
    eta = eta
  )
  
  # Measure runtime for GBM
  start_time <- Sys.time()
  gbm_model <- xgboost(params = params, data = train_matrix, nrounds = nrounds, verbose = 0)
  gbm_predictions <- predict(gbm_model, test_matrix)
  runtime <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
  
  # Calculate accuracy and misclassification rate
  accuracy <- mean(gbm_predictions == as.numeric(test_labels) - 1)
  return(list(accuracy = accuracy, runtime = runtime))
}

# Function to evaluate model consistency across different seeds
evaluate_model_consistency <- function(model_function, seeds, train, test_features, test_labels, sample_size, ...) {
  accuracies <- c()
  
  for (seed in seeds) {
    set.seed(seed)
    # Re-sample data with each seed
    sample_data <- train[sample(.N, sample_size)]
    result <- model_function(sample_data, ...)
    accuracies <- c(accuracies, result$accuracy)
  }
  
  # Calculate mean and standard deviation of accuracies
  mean_accuracy <- mean(accuracies)
  sd_accuracy <- sd(accuracies)
  
  return(list(mean_accuracy = mean_accuracy, sd_accuracy = sd_accuracy, accuracies = accuracies))
}
```

```{r load_data}
train <- fread("MNIST-fashion training set-49.csv", verbose = F)
test <- fread("MNIST-fashion testing set-49.csv", verbose = F)
```

```{r explore_data, eval = FALSE}

```


```{r clean_data}
# Ensure the labels are factors
train$label <- as.factor(train$label)
test$label <- as.factor(test$label)

```

```{r variables}
# Separate features and labels for the test set
test_features <- as.data.frame(test[, -1, with = FALSE])  # Exclude the label column for features
test_labels <- test$label  # Extract the label column as test labels

# Sample a subset of the training data
sample_data <- train[sample(.N, sample_size)]
train_features <- as.matrix(sample_data[, -1, with = FALSE])  # Features without the label
train_labels <- sample_data$label  # Labels

rf_train_features <- as.data.frame(sample_data[, -1, with = FALSE])  # Features without the label
rf_train_labels <- sample_data$label  # Labels
```

# {.tabset}

## Introduction

In this report, we explore the performance of various machine learning models applied to the MNIST Fashion dataset, with a focus on identifying the best-performing model for product classification. The primary aim is to select a model that excels in both accuracy and efficiency across multiple fashion product types, assessing each model's suitability for this classification task.

The report includes additional analyses designed to evaluate the model’s performance and uncover deeper insights within the data.

## Predictive Accuracy by Product

This table below displays the predictive accuracy of the model on each type of fashion product. Here’s a breakdown of the information and its implications:

-   **High Accuracy Categories**: The model performs exceptionally well on items like "Bag", "Trouser", and "Ankle boot". This shows that these products have distinctive features that the model can reliably recognize. High accuracy in these categories suggests the model's effectiveness in identifying items with clear visual characteristics, likely due to unique shapes or textures that set them apart from other items.

-   **Moderate Accuracy Categories**: Categories such as "Dress", "Sandal", and "Sneaker" also show strong performance, though slightly lower than the previous items. The model’s reasonable accuracy in these categories implies that it can generally distinguish these items but may face occasional challenges due to overlapping features with other categories.

-   **Low Accuracy Categories**: The model struggles with items such as "T-shirt/top", "Coat", "Pullover" and "Shirt". These lower accuracy rates suggest that these items may lack unique visual features, making them more difficult for the model to classify correctly. Shirts and pullovers might share visual similarities with other clothing types, which could lead to higher misclassification rates.

For low accuray categories, we recommend either improving model performance through additional training data or refining features to capture subtle distinctions. Another approach could be a hybrid model, where lower-confidence classifications are flagged for manual review. This hybrid approach ensures quality control without compromising automation benefits.

As product lines expand or new product categories are introduced, the model will need regular updates to maintain high accuracy. Establishing an ongoing data collection and model re-training process will ensure the classification system remains robust and scalable, supporting the business’s growth and product diversification.

```{r Accuracy_table}
rf_model <- randomForest(label ~ ., data = sample_data, ntree = rf_ntree)
rf_predictions <- predict(rf_model, newdata = test_features)

accuracy_per_product <- data.frame(test_labels, rf_predictions) %>%
  group_by(test_labels) %>%
  summarize(
    correctly_classified = sum(test_labels == rf_predictions),
    total_cases = n(),
    accuracy = round(100 * correctly_classified / total_cases, dp)
  )%>%
  arrange(desc(accuracy))

colnames(accuracy_per_product)[1] <- "Product Type"
kable(accuracy_per_product, col.names = c("Product Type", "Correctly Classified", "Total Cases", "Accuracy (%)"))
```

## Independent Investigation - Misclassification {.tabset}

Question: Which categories or classes are most frequently misclassified, and what are the common misclassification patterns?

Objective: Analyze the confusion matrix to identify which categories are often mistaken for each other. For example, certain fashion items might be similar in appearance and therefore more challenging to differentiate.

Models: We will focus on the top three performing models: Random Forest, SVM, and XGBoost

Value: Understanding misclassification patterns can help the client identify problematic categories and potentially improve image collection standards (e.g., lighting, angles) or preprocessing methods. This analysis can also inform the selection of more advanced models or techniques for problematic classes.


### Random Forest
```{r randomforest_input}
# Run Random Forest and measure runtime
start_time <- Sys.time()
rf_model <- randomForest(label ~ ., data = sample_data, ntree = rf_ntree)
rf_predictions <- predict(rf_model, newdata = test_features)
runtime <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))

# Convert rf_predictions to a factor and ensure it has the same levels as test_labels
rf_predictions <- factor(rf_predictions, levels = levels(test_labels))
test_labels <- factor(test_labels)  # Ensure test_labels is also a factor

# Generate confusion matrix
conf_matrix <- confusionMatrix(rf_predictions, test_labels)

# Generate accuracy
accuracy <- conf_matrix$overall['Accuracy']
```

Random Forest achieved an accuracy of `r accuracy`. Here are the confusion matrix and statistics:
```{r randomforest_output}
kable(conf_matrix$table)
plot_confusion(conf_matrix)
byClass(conf_matrix)
```


### KNN

```{r knn_input}
# Run KNN and measure runtime
start_time <- Sys.time()
knn_predictions <- knn(train = train_features, test = as.matrix(test_features), cl = train_labels, k = k)
runtime <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))

# Convert knn_predictions to a factor and ensure it has the same levels as test_labels
knn_predictions <- factor(knn_predictions, levels = levels(test_labels))
test_labels <- factor(test_labels)  # Ensure test_labels is also a factor

# Generate confusion matrix
conf_matrix <- confusionMatrix(knn_predictions, test_labels)

# Generate accuracy
accuracy <- conf_matrix$overall['Accuracy']
```

KNN achieved an accuracy of `r accuracy`. Here are the confusion matrix and statistics:
```{r knn_output}
kable(conf_matrix$table)
plot_confusion(conf_matrix)
byClass(conf_matrix)
```


### XGBoost

```{r xgboost_input}
# Convert the test set into a matrix and use unique variable names
xgb_test_features <- as.matrix(test[, -1, with = FALSE])  # Convert to matrix, excluding the label column
xgb_test_labels <- as.numeric(test$label) - 1             # XGBoost expects labels starting at 0

# Sample a subset of the training data for XGBoost with unique variable names
xgb_sample_data <- train[sample(.N, sample_size)]
xgb_train_features <- as.matrix(xgb_sample_data[, -1, with = FALSE])  # Convert to matrix, excluding the label column
xgb_train_labels <- as.numeric(xgb_sample_data$label) - 1             # Labels starting at 0 for XGBoost

# Convert the features and labels to DMatrix format for XGBoost
xgb_train_matrix <- xgb.DMatrix(data = xgb_train_features, label = xgb_train_labels)
xgb_test_matrix <- xgb.DMatrix(data = xgb_test_features, label = xgb_test_labels)

# Set XGBoost parameters
params <- list(
  objective = "multi:softmax",           # Multi-class classification
  num_class = length(unique(train$label)), # Number of classes
  max_depth = xgboost_max_depth,         # Max tree depth
  eta = xgboost_eta                      # Learning rate
)

# Run XGBoost and measure runtime
start_time <- Sys.time()
xgb_model <- xgboost(
  params = params,
  data = xgb_train_matrix,
  nrounds = xgboost_nrounds,
  verbose = 0
)
runtime <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))

# Make predictions on the test set
xgb_predictions <- predict(xgb_model, xgb_test_matrix)

# Convert the predictions and labels from numeric to factor with class names
xgb_predictions <- factor(xgb_predictions, levels = 0:9, labels = class_names)
xgb_test_labels <- factor(xgb_test_labels, levels = 0:9, labels = class_names)

# Generate the confusion matrix with class names
conf_matrix <- confusionMatrix(xgb_predictions, xgb_test_labels)

# Generate accuracy
accuracy <- conf_matrix$overall['Accuracy']

```

XGBoost achieved an accuracy of `r accuracy`. Here are the confusion matrix and statistics:
```{r xgboost_output}
kable(conf_matrix$table)
plot_confusion(conf_matrix)
byClass(conf_matrix)
```

### Conclusion

Summary of Key Findings:

* Frequent Confusion Pairs:
  + Ankle Boots and Sneakers: 216 misclassifications in total, highlighting a need for further distinction between these similar footwear styles.
  + Coats and Pullovers: 429 misclassifications, showing a high rate of confusion likely due to their similar appearance and texture.
  + Shirts and T-shirts/Tops: 317 misclassifications, indicating consistent difficulty across models in differentiating these upper-body clothing items.
  
* Model-Specific Insights:
  + XGBoost shows the lowest misclassification rates across the board, indicating better handling of nuanced visual differences.
  + Random Forest generally has a higher misclassification count compared to KNN and XGBoost, suggesting it may be more prone to errors on closely related clothing items.

* Recommendations:
  + Given the observed challenges, the client could benefit from exploring further image preprocessing or augmenting the dataset with more varied images for these commonly confused categories.
  + For more complex and subtle distinctions, the client might consider trying deep learning models (e.g., CNNs) specifically tailored to handle visual nuances if computationally feasible.


## Independent Investigation - Model Consistency 

Question: How consistent are the top three models with the lowest point score? 

Objective: Determine the consistency of the top three performing models by analyzing their performance using multiple different seeds. 

Models: We will focus on the top three performing models: Random Forest, SVM, and XGBoost

Value: By analyzing model consistency we can identify which model is most stable and reliable. In addition to determining the highest-performing model we also want to ensure that the model we select is dependable. If a model's performance relies greatly on the seed that has been set then it may in fact produce unpredictable and unreliable results which poses problems for decision making in the company. Additional, models that have lower variability and are consistent may be more robust to changes in data, therefore it can more reliably be applied to new and future data, and more informed decisions can be made. Overall, this analysis will allow for greater confidence in our final recommendations. 


```{r seed2}
# Set different seeds for testing consistency
seeds <- c(72, 101, 123, 456, 789)
```


```{r RF_consistency}
rf_consistency <- evaluate_model_consistency(
  run_random_forest, 
  seeds, 
  train, 
  test_features, 
  test_labels, 
  sample_size = 5000,  
  ntree = rf_ntree
)
```


```{r XGB_consistency}
xgb_consistency <- evaluate_model_consistency(
  run_gbm, 
  seeds, 
  train, 
  test_features, 
  test_labels, 
  sample_size = 5000, 
  max_depth = xgboost_max_depth, 
  eta = xgboost_eta, 
  nrounds = xgboost_nrounds
)
```


```{r SVM_consistency}
svm_consistency <- evaluate_model_consistency(run_svm, seeds, train, test_features, test_labels, sample_size = 5000, cost = 1, kernel = "linear")

svm_consistency <- evaluate_model_consistency(
  run_svm, 
  seeds, 
  train, 
  test_features, 
  test_labels, 
  sample_size = 5000, 
  cost = 1, 
  kernel = "linear"
)
```

```{r consistency_results}
# Create a data frame to display results as a datatable
consistency_results <- data.frame(
  Model = c("Random Forest", "SVM", "XGBoost"),
  Mean_Accuracy = c(
    round(rf_consistency$mean_accuracy, 4),
    round(svm_consistency$mean_accuracy, 4),
    round(xgb_consistency$mean_accuracy, 4)
  ),
  SD_Accuracy = c(
    round(rf_consistency$sd_accuracy, 4),
    round(svm_consistency$sd_accuracy, 4),
    round(xgb_consistency$sd_accuracy, 4)
  ),
  stringsAsFactors = FALSE
)

# Display consistency results 
datatable(
  consistency_results,
  options = list(
    ordering = TRUE,   # Enable sorting
    searching = FALSE, # Disable search
    lengthChange = FALSE, # Disable the ability to change page length
    paging = FALSE,    # Disable pagination
    info = FALSE       # Disable table information display
  )
)
```

Summary of Key Findings:

* Mean Accuracy:
  + XGBoost has the highest mean accuracy (0.8376), suggesting, on average, it performs the best out of the top three models. 
  + Random Forest follows very closely behind XGBoost in terms of average mean accuracy (0.8267). 
  + SVM has the lowest mean accuracy (0.8152). 

* Standard Deviation:
  + XGBoost has the lowest standard deviation (0.0020), indicating that of the three models it is the most consistent and stable when the seeds are changed. 
  + Random Forest also has a low standard deviation (0.0023) but slightly higher than XGBoost's. 
  + SVM has the highest standard deviation of the three models (0.0034), indicating more variability.

* Recommendations:
  + Based on the Mean Accuracy and Standard Deviations across seed changes, XGBoost is the best performing model of the top 3 models. Therefore, it will generate the most reliable results for the company. However, it does take the longest time to run. 
  + Random Forest is a close second when it comes to Mean Accuracy and Standard Deviations, and it also takes less time to run, making it a good alternative to XGBoost. 
  + SVM shows the most variability of the three models, making it less consistent and therefore less reliable. To improve the variability of this model, additional data processing may be beneficial.



## Independent Investigation - Preprocessing the Data Prior to Modeling 

Question: How would pre - processing the data affect model scoring? 

Objective: Preprocess the data by normalizing to demonstrate how doing work on the data prior to modeling can affect modeling results. 

Models: We will focus on Random Forest, SVM, and Neural Networks. 

Value: By adding a preliminary step we can discover if more work being performed on the raw data is worth the time to pursue in order to achieve more stable and higher performing results for certain models. 


####  Normalization - Model Results
```{r normal_scoreboard}
# Initialize a data frame to store results
scoreboard_normal <- data.frame(
  Model = character(),
  Sample_Size = integer(),
  Data = character(),
  A = numeric(),
  B = numeric(),
  C = numeric(),
  Points = numeric(),
  stringsAsFactors = FALSE
)
```


```{r normal_features}
train_normal <- train
test_normal <- test

# Assuming the first column is `label` and the rest are pixel values
pixel_columns <- names(train)[-1]  # Select all columns except `label`

# Normalize each pixel column to the range [0, 1]
train_normal[, (pixel_columns) := lapply(.SD, function(x) x / 255), .SDcols = pixel_columns]
test_normal[, (pixel_columns) := lapply(.SD, function(x) x / 255), .SDcols = pixel_columns]

#update test_features with normal values 
test_features <- as.data.frame(test_normal[, -1, with = FALSE]) 
```


```{r normal_models}

#train SVM 
for (n in n.values.normal) {
  for (i in 1:iterations) {
    sample_data <- train_normal[sample(.N, n)]
    result <- run_svm(sample_data, cost = 1, kernel = "linear")
    accuracy <- result$accuracy
    runtime <- result$runtime
    
    A <- n / total_train_size
    B <- min(1, runtime / hour)
    C <- 1 - accuracy
    points <- calculate_score(A, B, C)
    
    scoreboard_normal <- rbind(scoreboard_normal, data.frame(
      Model = "SVM, cost 1",
      Sample_Size = n,
      Data = paste("dat", n, i, sep = "_"),
      A = round(A, dp),
      B = round(B, dp),
      C = round(C, dp),
      Points = round(points, dp)
    ))
  }
}

#train random forest, 100 trees
for (n in n.values.normal) {
  for (i in 1:iterations) {
    sample_data <- train_normal[sample(.N, n)]
    result <- run_random_forest(sample_data, ntree = rf_ntree)
    accuracy <- result$accuracy
    runtime <- result$runtime
    
    A <- n / total_train_size
    B <- min(1, runtime / hour)
    C <- 1 - accuracy
    points <- calculate_score(A, B, C)
    
    scoreboard_normal<- rbind(scoreboard_normal, data.frame(
      Model = paste("Random Forest", "-", rf_ntree, " trees"),
      Sample_Size = n,
      Data = paste("dat", n, i, sep = "_"),
      A = round(A, dp),
      B = round(B, dp),
      C = round(C, dp),
      Points = round(points, dp)
    ))
  }
}

#neural network
for (n in n.values.normal) {
  for (i in 1:iterations) {
    sample_data <- train_normal[sample(.N, n)]
    result <- run_neural_net(sample_data)
    accuracy <- result$accuracy
    runtime <- result$runtime
    
    A <- n / total_train_size
    B <- min(1, runtime / hour)
    C <- 1 - accuracy
    points <- calculate_score(A, B, C)
    
   scoreboard_normal<- rbind(scoreboard_normal, data.frame(
      Model = "Neural Network",
      Sample_Size = n,
      Data = paste("dat", n, i, sep = "_"),
      A = round(A, dp),
      B = round(B, dp),
      C = round(C, dp),
      Points = round(points, dp)
    ))
  }
}

```

```{r normal_scoreboard_display}

scoreboard_normal <- scoreboard_normal[order(scoreboard_normal$Points, scoreboard_normal$Model, scoreboard_normal$Sample_Size), ]
datatable(
  scoreboard_normal,
  options = list(
    ordering = TRUE,   # Enable sorting
    searching = TRUE, 
    lengthChange = TRUE, 
    paging = TRUE,   
    info = FALSE       
  )
)
```


Summary of Key Findings:

* Improved Neural Networks:
  + The effects of normalization differ between different models. In terms of overall scoring, Neural Networks performed much better than prior to normalization, with the model scoring much lower. Runtime has significantly decreased for all neural networks. If the data was normalized in the main analysis, neural network's rank would increase greatly. 
  + Scoring for Random Forest is generally around the same values. SVM, however, did improve slightly. 

* Recommendations:
  + It is worth investing time into exploring processing the raw data prior to modeling. Certain models, such as neural networks in this case, can improve immensely when the raw data is altered slightly. By exploring steps to process the data, model performance can improve greatly, leading to a more efficient use of time. 

